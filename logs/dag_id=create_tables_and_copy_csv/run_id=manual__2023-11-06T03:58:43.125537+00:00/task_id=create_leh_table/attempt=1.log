[2023-11-06T03:58:47.443+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: create_tables_and_copy_csv.create_leh_table manual__2023-11-06T03:58:43.125537+00:00 [queued]>
[2023-11-06T03:58:47.481+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: create_tables_and_copy_csv.create_leh_table manual__2023-11-06T03:58:43.125537+00:00 [queued]>
[2023-11-06T03:58:47.484+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-11-06T03:58:47.552+0000] {taskinstance.py:1327} INFO - Executing <Task(SnowflakeOperator): create_leh_table> on 2023-11-06 03:58:43.125537+00:00
[2023-11-06T03:58:47.576+0000] {standard_task_runner.py:57} INFO - Started process 509 to run task
[2023-11-06T03:58:47.586+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'create_tables_and_copy_csv', 'create_leh_table', 'manual__2023-11-06T03:58:43.125537+00:00', '--job-id', '69', '--raw', '--subdir', 'DAGS_FOLDER/copy_processed_data_to_snowflake.py', '--cfg-path', '/tmp/tmpick9nw9o']
[2023-11-06T03:58:47.596+0000] {standard_task_runner.py:85} INFO - Job 69: Subtask create_leh_table
[2023-11-06T03:58:47.770+0000] {task_command.py:410} INFO - Running <TaskInstance: create_tables_and_copy_csv.create_leh_table manual__2023-11-06T03:58:43.125537+00:00 [running]> on host 0ca024afdc5f
[2023-11-06T03:58:48.283+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='create_tables_and_copy_csv' AIRFLOW_CTX_TASK_ID='create_leh_table' AIRFLOW_CTX_EXECUTION_DATE='2023-11-06T03:58:43.125537+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-11-06T03:58:43.125537+00:00'
[2023-11-06T03:58:48.285+0000] {sql.py:265} INFO - Executing: 
        CREATE TABLE IF NOT EXISTS leh (
            /* Define your schema here */
        );
    
[2023-11-06T03:58:48.317+0000] {base.py:73} INFO - Using connection ID 'snowflake_default' for task execution.
[2023-11-06T03:58:48.450+0000] {providers_manager.py:593} WARNING - The connection type 'aws' is already registered in the package 'apache-***-providers-amazon' with different class names: '***.providers.amazon.aws.hooks.base_aws.AwsBaseHook' and '***.providers.amazon.aws.hooks.redshift.RedshiftDataHook'. 
[2023-11-06T03:58:48.960+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.7/site-packages/snowflake/connector/options.py:109: UserWarning: You have an incompatible version of 'pyarrow' installed (6.0.1), please install a version that adheres to: 'pyarrow<10.1.0,>=10.0.1; extra == "pandas"'
  "pyarrow", installed_pyarrow_version, pandas_pyarrow_extra

[2023-11-06T03:58:49.196+0000] {base.py:73} INFO - Using connection ID 'snowflake_default' for task execution.
[2023-11-06T03:58:49.205+0000] {connection.py:300} INFO - Snowflake Connector for Python Version: 3.0.4, Python Version: 3.7.13, Platform: Linux-6.4.16-linuxkit-x86_64-with-debian-11.4
[2023-11-06T03:58:49.207+0000] {connection.py:1013} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2023-11-06T03:58:49.208+0000] {connection.py:1030} INFO - Setting use_openssl_only mode to False
[2023-11-06T03:58:50.692+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/auth/_auth.py", line 255, in authenticate
    socket_timeout=auth_timeout,
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/network.py", line 729, in _post_request
    _include_retry_params=_include_retry_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/network.py", line 819, in fetch
    session, method, full_url, headers, data, retry_ctx, **kwargs
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/network.py", line 941, in _request_exec_wrapper
    raise e
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/network.py", line 868, in _request_exec_wrapper
    **kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/network.py", line 1138, in _request_exec
    raise err
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/network.py", line 1063, in _request_exec
    raise ForbiddenError
snowflake.connector.errors.ForbiddenError: HTTP 403: Forbidden

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/common/sql/operators/sql.py", line 277, in execute
    **extra_kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 373, in run
    with closing(self.get_conn()) as conn:
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 296, in get_conn
    conn = connector.connect(**conn_config)
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/__init__.py", line 51, in Connect
    return SnowflakeConnection(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/connection.py", line 319, in __init__
    self.connect(**kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/connection.py", line 590, in connect
    self.__open_connection()
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/connection.py", line 860, in __open_connection
    self.authenticate_with_retry(self.auth_class)
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/connection.py", line 1127, in authenticate_with_retry
    self._authenticate(auth_instance)
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/connection.py", line 1165, in _authenticate
    session_parameters=self._session_parameters,
  File "/home/airflow/.local/lib/python3.7/site-packages/snowflake/connector/auth/_auth.py", line 268, in authenticate
    sqlstate=SQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED,
snowflake.connector.errors.ForbiddenError: 250001 (08001): None: Failed to connect to DB. Verify the account name is correct: EOB07055.snowflakecomputing.com:443. HTTP 403: Forbidden
[2023-11-06T03:58:50.709+0000] {taskinstance.py:1350} INFO - Marking task as FAILED. dag_id=create_tables_and_copy_csv, task_id=create_leh_table, execution_date=20231106T035843, start_date=20231106T035847, end_date=20231106T035850
[2023-11-06T03:58:50.739+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 69 for task create_leh_table (250001 (08001): None: Failed to connect to DB. Verify the account name is correct: EOB07055.snowflakecomputing.com:443. HTTP 403: Forbidden; 509)
[2023-11-06T03:58:50.770+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-11-06T03:58:50.807+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
